{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "973157d2-9249-48bf-bf38-7624402ac94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import albumentations\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as pyplot\n",
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import utils as vutils\n",
    "from discriminator import Discriminator\n",
    "from lpips import LPIPS\n",
    "from vqgan import VQGAN\n",
    "from utils import load_data, weights_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4da6df5d-da2a-4435-86c3-3af5d0dbac38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagePath(Dataset):\n",
    "    def __init__(self, path, size=None):\n",
    "        self.size = size\n",
    "\n",
    "        self.images = [os.path.join(path, file) for file in os.listdir(path)]\n",
    "        self._lenght = len(self.images)\n",
    "\n",
    "        self.rescaler = albumentations.SmallestMaxSize(max_size=self.size)\n",
    "        self.cropper = albumentations.CenterCrop(height=self.size, width=self.size)\n",
    "        self.preprocessor = albumentations.Compose([self.rescaler, self.cropper])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._lenght\n",
    "\n",
    "    def preprocess_image(self, image_path):\n",
    "        image = Image.open(image_path)\n",
    "        if not image.mode == 'RGB':\n",
    "            image = image.convert(\"RGB\")\n",
    "        image = np.array(image).astype(np.unit8)\n",
    "        image = self.preprocessor(image=image)['image']\n",
    "        image = (image / 127.5 - 1.0).astype(np.float32)\n",
    "        image = image.transpose(2, 0, 1)\n",
    "\n",
    "        return image\n",
    "\n",
    "def load_data(args):\n",
    "    train_data = ImagePath(args.dataset_path, size=256)\n",
    "    train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=False)\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ad86b67-42b4-45dd-b527-def12409bd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "    parser = argparse.ArgumentParser(description=\"VQGAN\")\n",
    "    parser.add_argument('--latent-dim', type=int, default=256, help='Latent dimension n_z (default: 256)')\n",
    "    parser.add_argument('--image-size', type=int, default=256, help='Image height and width (default: 256)')\n",
    "    parser.add_argument('--num-codebook-vectors', type=int, default=1024, help='Number of codebook vectors (default: 256)')\n",
    "    parser.add_argument('--beta', type=float, default=0.25, help='Commitment loss scalar (default: 0.25)')\n",
    "    parser.add_argument('--image-channels', type=int, default=3, help='Number of channels of images (default: 3)')\n",
    "    parser.add_argument('--dataset-path', type=str, default='/data', help='Path to data (default: /data)')\n",
    "    parser.add_argument('--device', type=str, default=\"cuda\", help='Which device the training is on')\n",
    "    parser.add_argument('--batch-size', type=int, default=6, help='Input batch size for training (default: 6)')\n",
    "    parser.add_argument('--epochs', type=int, default=100, help='Number of epochs to train (default: 50)')\n",
    "    parser.add_argument('--learning-rate', type=float, default=2.25e-05, help='Learning rate (default: 0.0002)')\n",
    "    parser.add_argument('--beta1', type=float, default=0.5, help='Adam beta param (default: 0.0)')\n",
    "    parser.add_argument('--beta2', type=float, default=0.9, help='Adam beta param (default: 0.999)')\n",
    "    parser.add_argument('--disc-start', type=int, default=10000, help='When to start the discriminator (default: 0)')\n",
    "    parser.add_argument('--disc-factor', type=float, default=1., help='')\n",
    "    parser.add_argument('--rec-loss-factor', type=float, default=1., help='Weighting factor for reconstruction loss.')\n",
    "    parser.add_argument('--perceptual-loss-factor', type=float, default=1., help='Weighting factor for perceptual loss.')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.dataset_path = r\"C:\\Users\\pedro\\OneDrive\\Área de Trabalho\\vq_gan\\flowers\"\n",
    "\n",
    "    train_vqgan = TrainVQGAN(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0b97298-f564-402e-85e8-8910da17b645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\pedro\\\\OneDrive\\\\Área de Trabalho\\\\vq_gan\\\\flowers'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25d6d1c-34a8-4204-815a-f4e7cbba6c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_data(datap)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
